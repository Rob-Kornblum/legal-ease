name: Model Performance Check

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        cd backend
        pip install -r requirements.txt
        
    - name: Wait for Render deployment and run evaluation
      run: |
        cd backend
        echo "Waiting for Render deployment to complete..."
        # Wait up to 5 minutes for Render deployment
        python -c "
        import requests
        import time
        import sys
        import os
        
        base_url = 'https://legal-ease-welcome.onrender.com'
        max_wait = 300  # 5 minutes
        check_interval = 15  # Check every 15 seconds
        
        print(f'Waiting for server at {base_url} to be ready...')
        print('This may take 1-2 minutes for Render deployment to complete.')
        
        for i in range(0, max_wait, check_interval):
            try:
                response = requests.get(f'{base_url}/health', timeout=10)
                if response.status_code == 200:
                    print(f'✅ Server is ready after {i} seconds!')
                    break
            except Exception as e:
                pass
            
            if i + check_interval < max_wait:
                print(f'Still waiting... ({i + check_interval}s/{max_wait}s)')
                time.sleep(check_interval)
        else:
            print(f'❌ Server not ready after {max_wait} seconds')
            sys.exit(1)
        "
        
        # Now run the evaluation
        python enhanced_eval.py
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        API_URL: https://legal-ease-welcome.onrender.com
        
    - name: Check minimum performance
      run: |
        cd backend
        python -c "
        import yaml
        import sys
        import json
        import subprocess
        
        # Run the evaluation and capture results
        try:
            result = subprocess.run(['python', 'enhanced_eval.py'], 
                                  capture_output=True, text=True, check=True)
            output = result.stdout
            
            # Parse accuracy from output
            for line in output.split('\n'):
                if 'Category Accuracy:' in line:
                    parts = line.split(':')[-1].strip()
                    correct, total = parts.split(' ')[0].split('/')
                    accuracy = float(correct) / float(total)
                    break
            else:
                print('Could not parse accuracy from output')
                sys.exit(1)
            
            min_accuracy = 0.90
            print(f'Accuracy: {accuracy:.1%}, Required: {min_accuracy:.1%}')
            
            if accuracy < min_accuracy:
                print(f'❌ Performance below threshold!')
                sys.exit(1)
            else:
                print(f'✅ Performance meets requirements!')
                
        except subprocess.CalledProcessError as e:
            print(f'❌ Evaluation script failed: {e}')
            print(f'stdout: {e.stdout}')
            print(f'stderr: {e.stderr}')
            sys.exit(1)
        "
